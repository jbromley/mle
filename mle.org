#+TITLE: Machine Learning Engineer
#+AUTHOR: J. Bromley <jbromley@gmail.com>

* Information
** Welcome to Machine Learning
*** Projects
    - Model Evaluation Validation: Predicting Boston Housing Prices
    - Supervised Learning: Finding Donors for CharityML
    - Unsupervised Learning: Creating Custom Segments
*** Program Structure
**** Machine Learning Foundations
**** Model Evaluation and Validation
     - training
     - determining goodness of a model
     - improving models
**** Supervised Learning
     - linear and logistic regression
     - decision trees
     - naive Bayes
     - neural networks
     - support vector machines
**** Unsupervised Learning
     - clustering
     - dimensionality reduction
** What is machine learning?
** Training and testing models
Learn about measurement tools that tell us how well our algorithms and working
and how to make them better.
*** Decision Tree
Split on most influential factor, then successively on less influential.
*** Naive Bayes (e.g. spam detector)
*** Logistic regression
Error function is log loss probability. The class boundary is a line.
*** Support vector machines
Consider points closest to the class boundary. We want the boundary to be as 
far away from all points as possible.
*** Neural networks
A single line may not be a good boundary between classes. We can then use two
lines.
*** Kernel method
Sometimes a set of lines isn't enough to form a good class boundary. Then we 
can use kernel methods.
*** K-means clustering
We must know the number of clusters.
*** Hierarchical clustering
We don't know how many clusters, but we know a good distance criteria for 
splitting points into clusters.

#+NAME: Traing/testing split generator
#+BEGIN_SRC python
from sklean.model_selection import train_test_split
#+END_SRC
** Evaluation metrics
*** Confusion matrix
Lays out true positives, false positives, false negatives, and true negatives.
Note that false positives are also called type 1 errors (error of the first kind)
and false negatives are called type 2 errors (error of the second kind).
*** Accuracy
Ratio of correctly classified points to total points.

#+NAME: accuracy calculation
#+BEGIN_SRC python
from sklean.metrics import accuracy_score
#+END_SRC

Some times accuracy is not the best measure. For example, saying all credit card
transactions are good would give high accuracy (there are few fraudulent transactions)
but not catch any of the fraudulent transactions
*** False positives and false negatives
*** Precision and recall
Precision is true positives / (true positives + false positives). Look at 
the prediction/classification and see how many are correct.
Recall is true positives / ( true positives + false negatives). Look at all
of a class and see how many were correctly classified.
*** F1 Score
F1 = 2 * precision * recall / (precision + recall). This is the harmonic mean
of precision and recall. This tends to the smaller of the values.
*** F-beta Score
Allows us to prefer precision to recall or vice versa. Low beta prefers 
precision and high beta prefers recall.
F_B = (1 + B^2) * precision * recall / (B^2 * precision + recall)
*** Receiver Operating Characteristic
Perfect model has area under curve of 1. A good model should have an area of 
around 0.8 under the ROC curve, and a random split should give an area of
0.5.
*** Regression metrics
**** mean absolute error
**** mean squared error
**** R2 score
Compares a model to the simplest possible moddle. 
R2 = 1 - (MAE / MAE_simple)
** Model Selection
*** K-fold cross validation
#+NAME: K-fold cross validation with randomization
#+BEGIN_SRC python
from sklearn.model_selection import KFold
kf = KFold(12, 3, shuffle=True)
#+END_SRC
*** Grid search
#+NAME: Grid search in sklearn
#+BEGIN_SRC python
from sklean.model_selection import GridSearchCV

# Define hyperparameters to search over.
parameters = {"kernel": ["poly", "rbf"], "C": [0.1, 1.0, 10.0]}

# Create a scorer to rate hyperparameter combinations.
from sklearn.metrics import make_scorer
from sklearn.metrics import f1_score
scorer = make_scorer(f1_score)

# Create a grid search object.
grid_obj = GridSearchCV(clf, parameters, scoring=scorer)
grid_fit = grid_obj.fit(X, y)

# Get the best estimator.
best_clf = grid_fit.best_estimator_

# Use best_clf to make predictions.
#+END_SRC

* Tasks [18/29]
  :PROPERTIES:
  :ORDERED:  t
  :END:
** DONE Lesson 1: Welcome to Machine Learning
   :PROPERTIES:
   :Effort:   0:20
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-18 Wed 22:30]--[2018-07-18 Wed 22:50] =>  0:20
   :END:
** DONE Lesson 2: What is Machine Learning?
   :PROPERTIES:
   :Effort:   0:45
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-18 Wed 22:50]--[2018-07-18 Wed 23:33] =>  0:43
   :END:
** DONE Lesson 3: Introductory Practice Project
   :PROPERTIES:
   :Effort:   2:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-19 Thu 22:08]--[2018-07-19 Thu 22:53] =>  0:45
   CLOCK: [2018-07-19 Thu 00:20]--[2018-07-19 Thu 00:56] =>  0:36
   :END:
** DONE Lesson 4: Career Services Available to You
   :PROPERTIES:
   :Effort:   0:01
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-19 Thu 22:57]--[2018-07-19 Thu 22:58] =>  0:01
   :END:
** DONE Lesson 5: NumPy and pandas Assessment
   :PROPERTIES:
   :Effort:   0:10
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-19 Thu 22:59]--[2018-07-19 Thu 23:03] =>  0:04
   :END:
** DONE Lesson 6: Training and Testing Models
   :PROPERTIES:
   :Effort:   1:30
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-20 Fri 23:10]--[2018-07-20 Fri 23:51] =>  0:41
   CLOCK: [2018-07-20 Fri 01:17]--[2018-07-20 Fri 01:37] =>  0:43
   :END:
** DONE Lesson 7: Evaluation Metrics
   :PROPERTIES:
   :Effort:   0:30
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-21 Sat 23:19]--[2018-07-21 Sat 23:56] =>  0:37
   CLOCK: [2018-07-21 Sat 00:34]--[2018-07-21 Sat 00:50] =>  0:16
   CLOCK: [2018-07-20 Fri 23:52]--[2018-07-21 Sat 00:19] =>  0:27
   :END:
** DONE Lesson 8: Model Selection
   :PROPERTIES:
   :Effort:   1:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-25 Wed 10:40]--[2018-07-25 Wed 11:23] =>  0:43
   CLOCK: [2018-07-25 Wed 01:47]--[2018-07-25 Wed 02:20] =>  0:33
   :END:
** DONE Lesson 9: Model Evalutation and Validation Assessment
   :PROPERTIES:
   :Effort:   0:10
   :END:
** DONE Project: Predicting Boston Housing Prices [0/0]
   DEADLINE: <2018-08-07 Tue>
Things to learn by doing this project are
  - How to explore data and observe features.
  - How to train and test models.
  - How to identify potential problems, such as errors due to bias or variance.
  - How to apply techniques to improve the model, such as cross-validation and grid search.
** DONE Lesson 11: Linear Regression
   :PROPERTIES:
   :Effort:   2:00
   :END:
** DONE Lesson 12: Perceptron Algorithm
   :PROPERTIES:
   :Effort:   2:00
   :END:
** DONE Lesson 13: Decision Trees
   :PROPERTIES:
   :Effort:   2:00
   :END:
** DONE Lesson 14: Naive Bayes
   :PROPERTIES:
   :Effort:   2:00
   :ORDERED:  t
   :END:
   :LOGBOOK:
   CLOCK: [2018-09-07 Fri 9:15]--[2018-09-07 Fri 11:35] =>  2:20
   CLOCK: [2018-09-06 Thu 9:15]--[2018-09-06 Thu 10:15] =>  1:00
   :END:
** DONE Lesson 15: Support Vector Machines
   :PROPERTIES:
   :Effort:   2:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-09-14 Fri 08:30]--[2018-09-14 Fri 10:11] =>  1:41
   CLOCK: [2018-09-14 Fri 03:30]--[2018-09-14 Fri 04:08] =>  0:38
   :END:
** DONE Lesson 16: Ensemble Methods
   :PROPERTIES:
   :Effort:   1:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-09-14 Fri 09:20]--[2018-09-14 Fri 10:31] =>  1:11
   :END:
** DONE Lesson 17: Supervised Learning Assessment
   :PROPERTIES:
   :Effort:   0:20
   :END:
   :LOGBOOK:
   CLOCK: [2018-09-14 Fri 10:43]--[2018-09-14 Fri 10:53] =>  0:10
   :END:
** DONE Project: Supervised Learning Project
   DEADLINE: <2018-08-28 Tue>
   :LOGBOOK:
   CLOCK: [2018-09-21 Fri 08:51]--[2018-09-21 Fri 13:19] =>  4:28
   CLOCK: [2018-09-19 Wed 09:04]--[2018-09-19 Wed 10:36] =>  1:32
   :END:
*** Gaussian Naive Bayes (sklearn.naive_bayes.GaussianNB)
Better suited for continuous data that likely follows a Gaussian
distribution. For our use case, there are a lot of categorical features that
have been one-hot encoded, so this algorithm doesn't seem well-suited.
*** Decision Trees
**** Applications
     - Customer relations management
     - Detection of fraudulent financial statements
**** Advantages
     - Simple to understand and interpret
     - Can handle both numerical and categorical data
     - Requires little data preparation
     - Uses a white box model
     - Possible to validate a model using statistical tests
     - Performs well with large data sets
     - Mirrors human decision making more closely than other algorithms
**** Limitations
     - Trees may be non-robust to small changes in the training data
     - Finding an optimal decision tree is an NP-complete problem, so it cannot
       be guaranteed that an algorithm will return the optimal decision tree.
     - For data with categorical features with several levels, the information
       gain is biased towards categorical features with more levels.
*** Random Forest ([[http://amateurdatascientist.blogspot.com/2012/01/random-forest-algorithm.html][Machine Learning Madness: Random Forest Algorithm]])
**** Applications
     - Analysis of microarray gene expression data [fn:qi2012]
     - Genome-wide association studies
**** Advantages
     - Highly accurate for many data sets [fn:caruana2008]
     - Can work well with small data sets
     - Runs efficiently on large data sets
     - Can handle thousands of input variables
     - Can estimate what variables are important in the classification
**** Limitations
     - Not easily intrepretable
     - Prone to overfitting if tree depth other tree hyperparameters are not
       controlled
*** Support Vector Machines
**** Applications
     - One-class SVM for detecting outliers in hospital billing
     - Solvency analysis in banking [fn:auria2008]
**** Advantages
     - Has a regularization parameter to help avoid over-fitting
     - Uses the kernel trick, so expert knowledge about a problem can be
       designed into the kernel
     - Is defined by a convex optimization problem (i.e. no local minima), so
       there are efficient methods to solve SVMs and obtain a unique result
     - Scales relatively well to high dimensional data
**** Disadvantages
     - Lacks transparency of results and interpretability
     - Have a long training time for large data sets
*** Logistic Regression
**** Applications
     - Prediction of financial crises [fn:han2008]
**** Advantages    
     - Provides probability estimates for classifications
     - Dependent variables don't have to be normally distributed
     - No linear relationship between the dependent variables and independent
       variable is assumed
     - Can handle non-linear effects, interaction effects, and power terms
**** Disadvantages
     - Can be inefficient to train
     - May have collinearity problems
** TODO Lesson 19: Clustering
   :PROPERTIES:
   :Effort:   1:30
   :END:
** TODO Lesson 20: Clustering Mini-Project
   :PROPERTIES:
   :Effort:   1:00
   :END:
** TODO Lesson 21: Hierarchical and Density-based Clustering
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 22: Gaussian Mixture Models and Cluster Validation
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 23: Feature Scaling
   :PROPERTIES:
   :Effort:   0:30
   :END:
** TODO Lesson 24: PCA
   :PROPERTIES:
   :Effort:   1:15
   :END:
** TODO Lesson 25: PCA Mini-Project
   :PROPERTIES:
   :Effort:   1:00
   :END:
** TODO Lesson 26: Random Projection and ICA
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 27: Unsupervised Learning Assessment
   :PROPERTIES:
   :Effort:   0:10
   :END:
** TODO Project: Creating Custom Segments
   DEADLINE: <2018-09-18 Tue>
** TODO Lesson 30: Congratulations
   :PROPERTIES:
   :Effort:   0:01
   :END:


[fn:caruana2008] Caruana, Rich; Karampatziakis, Nikos; Yessenalina, Ainur (2008). 
"An empirical evaluation of supervised learning in high dimensions". [[http://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning][
Proceedings of the 25th International Conference on Machine Learning (ICML)]].


[fn:qi2012] Qi, Yanjun. "Random forest for bioinformatics." Ensemble machine
learning. Springer, Boston, MA, 2012. 307-323.


[fn:auria2008] Auria, Laura, and Rouslan A. Moro. "Support vector machines (SVM)
as a technique for solvency analysis." (2008).


[fn:han2008] Han, Dongmei, Liqun Ma, and Changrui Yu. "Financial Prediction:
Application of Logistic Regression with Factor Analysis." Wireless
Communications, Networking and Mobile Computing, 2008. WiCOM'08. 4th
International Conference on. IEEE, 2008.
