#+TITLE: Machine Learning Engineer
#+AUTHOR: J. Bromley <jbromley@gmail.com>

* Information
** Welcome to Machine Learning
*** Projects
    - Model Evaluation Validation: Predicting Boston Housing Prices
    - Supervised Learning: Finding Donors for CharityML
    - Unsupervised Learning: Creating Custom Segments
*** Program Structure
**** Machine Learning Foundations
**** Model Evaluation and Validation
     - training
     - determining goodness of a model
     - improving models
**** Supervised Learning
     - linear and logistic regression
     - decision trees
     - naive Bayes
     - neural networks
     - support vector machines
**** Unsupervised Learning
     - clustering
     - dimensionality reduction
** What is machine learning?
** Training and testing models
Learn about measurement tools that tell us how well our algorithms and working
and how to make them better.
*** Decision Tree
Split on most influential factor, then successively on less influential.
*** Naive Bayes (e.g. spam detector)
*** Logistic regression
Error function is log loss probability. The class boundary is a line.
*** Support vector machines
Consider points closest to the class boundary. We want the boundary to be as 
far away from all points as possible.
*** Neural networks
A single line may not be a good boundary between classes. We can then use two
lines.
*** Kernel method
Sometimes a set of lines isn't enough to form a good class boundary. Then we 
can use kernel methods.
*** K-means clustering
We must know the number of clusters.
*** Hierarchical clustering
We don't know how many clusters, but we know a good distance criteria for 
splitting points into clusters.

#+NAME: Traing/testing split generator
#+BEGIN_SRC python
from sklean.model_selection import train_test_split
#+END_SRC
** Evaluation metrics
*** Confusion matrix
Lays out true positives, false positives, false negatives, and true negatives.
Note that false positives are also called type 1 errors (error of the first kind)
and false negatives are called type 2 errors (error of the second kind).
*** Accuracy
Ratio of correctly classified points to total points.

#+NAME: accuracy calculation
#+BEGIN_SRC python
from sklean.metrics import accuracy_score
#+END_SRC

Some times accuracy is not the best measure. For example, saying all credit card
transactions are good would give high accuracy (there are few fraudulent transactions)
but not catch any of the fraudulent transactions
*** False positives and false negatives
*** Precision and recall
Precision is true positives / (true positives + false positives). Look at 
the prediction/classification and see how many are correct.
Recall is true positives / ( true positives + false negatives). Look at all
of a class and see how many were correctly classified.
*** F1 Score
F1 = 2 * precision * recall / (precision + recall). This is the harmonic mean
of precision and recall. This tends to the smaller of the values.
*** F-beta Score
Allows us to prefer precision to recall or vice versa. Low beta prefers 
precision and high beta prefers recall.
F_B = (1 + B^2) * precision * recall / (B^2 * precision + recall)
*** Receiver Operating Characteristic
Perfect model has area under curve of 1. A good model should have an area of 
around 0.8 under the ROC curve, and a random split should give an area of
0.5.
*** Regression metrics
**** mean absolute error
**** mean squared error
**** R2 score
Compares a model to the simplest possible moddle. 
R2 = 1 - (MAE / MAE_simple)
** Model Selection
*** K-fold cross validation
#+NAME: K-fold cross validation with randomization
#+BEGIN_SRC python
from sklearn.model_selection import KFold
kf = KFold(12, 3, shuffle=True)
#+END_SRC
*** Grid search
#+NAME: Grid search in sklearn
#+BEGIN_SRC python
from sklean.model_selection import GridSearchCV

# Define hyperparameters to search over.
parameters = {"kernel": ["poly", "rbf"], "C": [0.1, 1.0, 10.0]}

# Create a scorer to rate hyperparameter combinations.
from sklearn.metrics import make_scorer
from sklearn.metrics import f1_score
scorer = make_scorer(f1_score)

# Create a grid search object.
grid_obj = GridSearchCV(clf, parameters, scoring=scorer)
grid_fit = grid_obj.fit(X, y)

# Get the best estimator.
best_clf = grid_fit.best_estimator_

# Use best_clf to make predictions.
#+END_SRC

* Tasks [14/29]
  :PROPERTIES:
  :ORDERED:  t
  :END:
** DONE Lesson 1: Welcome to Machine Learning
   :PROPERTIES:
   :Effort:   0:20
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-18 Wed 22:30]--[2018-07-18 Wed 22:50] =>  0:20
   :END:
** DONE Lesson 2: What is Machine Learning?
   :PROPERTIES:
   :Effort:   0:45
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-18 Wed 22:50]--[2018-07-18 Wed 23:33] =>  0:43
   :END:
** DONE Lesson 3: Introductory Practice Project
   :PROPERTIES:
   :Effort:   2:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-19 Thu 22:08]--[2018-07-19 Thu 22:53] =>  0:45
   CLOCK: [2018-07-19 Thu 00:20]--[2018-07-19 Thu 00:56] =>  0:36
   :END:
** DONE Lesson 4: Career Services Available to You
   :PROPERTIES:
   :Effort:   0:01
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-19 Thu 22:57]--[2018-07-19 Thu 22:58] =>  0:01
   :END:
** DONE Lesson 5: NumPy and pandas Assessment
   :PROPERTIES:
   :Effort:   0:10
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-19 Thu 22:59]--[2018-07-19 Thu 23:03] =>  0:04
   :END:
** DONE Lesson 6: Training and Testing Models
   :PROPERTIES:
   :Effort:   1:30
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-20 Fri 23:10]--[2018-07-20 Fri 23:51] =>  0:41
   CLOCK: [2018-07-20 Fri 01:17]--[2018-07-20 Fri 01:37] =>  0:43
   :END:
** DONE Lesson 7: Evaluation Metrics
   :PROPERTIES:
   :Effort:   0:30
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-21 Sat 23:19]--[2018-07-21 Sat 23:56] =>  0:37
   CLOCK: [2018-07-21 Sat 00:34]--[2018-07-21 Sat 00:50] =>  0:16
   CLOCK: [2018-07-20 Fri 23:52]--[2018-07-21 Sat 00:19] =>  0:27
   :END:
** DONE Lesson 8: Model Selection
   :PROPERTIES:
   :Effort:   1:00
   :END:
   :LOGBOOK:
   CLOCK: [2018-07-25 Wed 10:40]--[2018-07-25 Wed 11:23] =>  0:43
   CLOCK: [2018-07-25 Wed 01:47]--[2018-07-25 Wed 02:20] =>  0:33
   :END:
** DONE Lesson 9: Model Evalutation and Validation Assessment
   :PROPERTIES:
   :Effort:   0:10
   :END:
** DONE Project: Predicting Boston Housing Prices [0/0]
   DEADLINE: <2018-08-07 Tue>
Things to learn by doing this project are
  - How to explore data and observe features.
  - How to train and test models.
  - How to identify potential problems, such as errors due to bias or variance.
  - How to apply techniques to improve the model, such as cross-validation and grid search.
** DONE Lesson 11: Linear Regression
   :PROPERTIES:
   :Effort:   2:00
   :END:
** DONE Lesson 12: Perceptron Algorithm
   :PROPERTIES:
   :Effort:   2:00
   :END:
** DONE Lesson 13: Decision Trees
   :PROPERTIES:
   :Effort:   2:00
   :END:
** DONE Lesson 14: Naive Bayes
   :PROPERTIES:
   :Effort:   2:00
   :ORDERED:  t
   :END:
   :LOGBOOK:
   CLOCK: [2018-09-07 Fri 9:15]--[2018-09-07 Fri 11:35] =>  2:20
   CLOCK: [2018-09-06 Thu 9:15]--[2018-09-06 Thu 10:15] =>  1:00
   :END:
** TODO Lesson 15: Support Vector Machines
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 16: Ensemble Methods
   :PROPERTIES:
   :Effort:   1:00
   :END:
** TODO Lesson 17: Supervised Learning Assessment
   :PROPERTIES:
   :Effort:   0:20
   :END:
** TODO Project: Supervised Learning Project [0/0]
   DEADLINE: <2018-08-28 Tue>
** TODO Lesson 19: Clustering
   :PROPERTIES:
   :Effort:   1:30
   :END:
** TODO Lesson 20: Clustering Mini-Project
   :PROPERTIES:
   :Effort:   1:00
   :END:
** TODO Lesson 21: Hierarchical and Density-based Clustering
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 22: Gaussian Mixture Models and Cluster Validation
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 23: Feature Scaling
   :PROPERTIES:
   :Effort:   0:30
   :END:
** TODO Lesson 24: PCA
   :PROPERTIES:
   :Effort:   1:15
   :END:
** TODO Lesson 25: PCA Mini-Project
   :PROPERTIES:
   :Effort:   1:00
   :END:
** TODO Lesson 26: Random Projection and ICA
   :PROPERTIES:
   :Effort:   2:00
   :END:
** TODO Lesson 27: Unsupervised Learning Assessment
   :PROPERTIES:
   :Effort:   0:10
   :END:
** TODO Project: Creating Custom Segments
   DEADLINE: <2018-09-18 Tue>
** TODO Lesson 30: Congratulations
   :PROPERTIES:
   :Effort:   0:01
   :END:

